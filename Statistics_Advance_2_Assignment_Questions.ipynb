{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x4T5-b56UlA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Explain the properties of the F-distribution.**"
      ],
      "metadata": {
        "id": "CE9IVn6i6cxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **F-distribution** is a continuous probability distribution used primarily in hypothesis testing, such as in ANOVA (Analysis of Variance), to compare variances.\n",
        "\n",
        "### Definition:\n",
        "It is the distribution of the ratio of two scaled chi-squared distributions, typically used for comparing two sample variances.\n",
        "\n",
        "### Formula:\n",
        "If \\( X_1^2 \\) and \\( X_2^2 \\) are independent chi-squared variables with \\( d_1 \\) and \\( d_2 \\) degrees of freedom, respectively, the F-statistic is given by:\n",
        "\\[\n",
        "F = \\frac{(X_1^2 / d_1)}{(X_2^2 / d_2)}\n",
        "\\]\n",
        "Where:\n",
        "- \\( d_1 \\) is the degrees of freedom of the numerator,\n",
        "- \\( d_2 \\) is the degrees of freedom of the denominator.\n",
        "\n",
        "### Key Points:\n",
        "- The F-distribution is used for variance ratio tests.\n",
        "- It is positively skewed and depends on two sets of degrees of freedom."
      ],
      "metadata": {
        "id": "LgrG4dTu6n3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?**"
      ],
      "metadata": {
        "id": "NwI5cddu6s9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **F-distribution** is used in several statistical tests where the ratio of variances is of interest. The key types of statistical tests that use the F-distribution are:\n",
        "\n",
        "### 1. **Analysis of Variance (ANOVA)**\n",
        "   - **Why it's appropriate**: ANOVA tests the hypothesis that the means of two or more groups are equal by comparing the variance between groups to the variance within groups. Since the test involves comparing ratios of variances, the F-distribution is used to determine whether the group means are significantly different.\n",
        "\n",
        "### 2. **Regression Analysis (F-test for overall significance)**\n",
        "   - **Why it's appropriate**: In regression, the F-test is used to assess the overall significance of a model. It compares the model’s explained variance to the unexplained variance. The F-distribution is used to test if the model fits the data better than a model with no predictors.\n",
        "\n",
        "### 3. **Testing the Equality of Variances**\n",
        "   - **Why it's appropriate**: The F-distribution is used to test if two populations have equal variances, which is important in several tests like comparing two sample variances. The ratio of the two sample variances follows an F-distribution under the null hypothesis of equal variances.\n",
        "\n",
        "### 4. **Comparing Nested Models in Regression**\n",
        "   - **Why it's appropriate**: In comparing two models (one nested within the other), the F-test is used to determine if the addition of extra variables significantly improves the model fit. The test uses the ratio of the sum of squared residuals of the models to follow the F-distribution.\n",
        "\n",
        "### Why F-distribution is appropriate:\n",
        "   - It is used because the ratio of two chi-squared distributions (which are involved in variance calculations) follows the F-distribution.\n",
        "   - The F-distribution is suitable when comparing variances and provides a way to assess whether observed differences are statistically significant."
      ],
      "metadata": {
        "id": "8cU0VAEc65Jx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?**"
      ],
      "metadata": {
        "id": "vvzQneBT7OlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To conduct an **F-test** to compare the variances of two populations, the following key assumptions must be met:\n",
        "\n",
        "### 1. **Independence of Samples**\n",
        "   - The two samples being compared must be independent of each other. This means that the data points in one sample should not influence the data points in the other sample.\n",
        "\n",
        "### 2. **Normality of Populations**\n",
        "   - The data in both populations should follow a **normal distribution**. The F-test assumes that the populations from which the samples are drawn are normally distributed. If the populations are not normal, the test might not be valid.\n",
        "\n",
        "### 3. **Random Sampling**\n",
        "   - The samples must be **randomly selected** from the populations to ensure that they are representative and unbiased.\n",
        "\n",
        "### 4. **Ratio of Variances**\n",
        "   - The test compares the ratio of the sample variances. Specifically, the larger sample variance should be placed in the numerator and the smaller in the denominator to ensure the F-statistic is positive.\n",
        "\n",
        "### 5. **Independent and Identically Distributed (i.i.d.) Data**\n",
        "   - The observations within each sample should be independent and identically distributed, ensuring each observation comes from the same underlying distribution.\n",
        "\n",
        "These assumptions ensure that the F-test produces reliable and valid results when comparing variances between two populations."
      ],
      "metadata": {
        "id": "Apu1T3XV7VGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What is the purpose of ANOVA, and how does it differ from a t-test? **"
      ],
      "metadata": {
        "id": "z7Wm7zQA7fxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Purpose of ANOVA (Analysis of Variance):**\n",
        "The purpose of **ANOVA** is to test whether there are any statistically significant differences between the means of **three or more groups**. It helps determine if at least one group mean is different from the others, based on the variation within and between the groups.\n",
        "\n",
        "- **Null Hypothesis (H₀)**: All group means are equal.\n",
        "- **Alternative Hypothesis (H₁)**: At least one group mean is different.\n",
        "\n",
        "### **How ANOVA works:**\n",
        "ANOVA compares the **variance between groups** to the **variance within groups**. If the variance between groups is significantly larger than the variance within groups, it suggests that the group means are different. The F-statistic is used to assess this ratio.\n",
        "\n",
        "---\n",
        "\n",
        "### **Difference between ANOVA and t-test:**\n",
        "\n",
        "| Aspect                     | **ANOVA**                                         | **t-test**                                          |\n",
        "|----------------------------|--------------------------------------------------|---------------------------------------------------|\n",
        "| **Purpose**                 | Compares means of **three or more groups**.      | Compares means of **two groups**.                 |\n",
        "| **Test Statistic**          | F-statistic (ratio of variances).                | t-statistic (difference of means).                |\n",
        "| **Hypothesis**              | Tests if at least one group mean differs.        | Tests if the means of two groups are equal.       |\n",
        "| **When to Use**             | When comparing the means of **three or more groups**. | When comparing the means of **two groups**.        |\n",
        "| **Multiple Comparisons**    | Can be extended to multiple groups.              | Can only compare two groups.                      |\n",
        "\n",
        "### **Key Difference:**\n",
        "- **ANOVA** is used when there are **more than two groups** to compare, whereas the **t-test** is specifically for comparing **two groups**.\n",
        "- If ANOVA shows significant differences, post-hoc tests (like Tukey's HSD) can be used to identify which specific groups differ from each other."
      ],
      "metadata": {
        "id": "LzShk0rE7mbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.**"
      ],
      "metadata": {
        "id": "VmWZSeYp7qj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **When to Use One-Way ANOVA Instead of Multiple t-tests:**\n",
        "\n",
        "You should use a **one-way ANOVA** when comparing the means of **three or more groups**. Instead of performing multiple pairwise t-tests between all group combinations, a one-way ANOVA provides a more efficient and statistically valid approach.\n",
        "\n",
        "### **Why One-Way ANOVA is Preferred Over Multiple t-tests:**\n",
        "\n",
        "1. **Controlling Type I Error:**\n",
        "   - **Multiple t-tests** increase the risk of a **Type I error** (false positive), where you mistakenly reject a true null hypothesis.\n",
        "   - Each t-test has a risk of making a Type I error (usually set at 0.05), so performing many t-tests (for example, 6 t-tests for 3 groups) increases the overall likelihood of a Type I error. This is known as the **familywise error rate**.\n",
        "   - **ANOVA**, on the other hand, controls the Type I error rate across all group comparisons in one test, maintaining the overall significance level (e.g., 0.05).\n",
        "\n",
        "2. **Efficiency:**\n",
        "   - Running multiple t-tests requires performing multiple comparisons, which increases the time and complexity of the analysis.\n",
        "   - **One-way ANOVA** is a more efficient way to determine whether there is a significant difference between group means without having to conduct numerous t-tests.\n",
        "\n",
        "3. **Holistic Comparison:**\n",
        "   - **ANOVA** tests the overall hypothesis that at least one group mean is different from the others, rather than testing specific pairs of groups individually.\n",
        "   - If ANOVA indicates a significant result, post-hoc tests (e.g., Tukey's HSD) can be used to identify which groups differ.\n",
        "\n",
        "### **Example:**\n",
        "Suppose you have three groups (A, B, and C) and want to compare their means. Using multiple t-tests:\n",
        "- First, compare A vs. B,\n",
        "- Then B vs. C,\n",
        "- And finally A vs. C.\n",
        "\n",
        "Each comparison has a risk of Type I error, and the overall error rate increases. Using **one-way ANOVA**, you test all groups in a single analysis, reducing the chance of an error and making the process more efficient.\n",
        "\n",
        "### **Conclusion:**\n",
        "- **One-way ANOVA** is used when comparing **more than two groups** to minimize the risk of Type I error and to simplify the analysis.\n",
        "- It provides a more reliable method to test if there are any differences among the groups as a whole, and if significant, post-hoc tests can pinpoint where the differences lie."
      ],
      "metadata": {
        "id": "1bp9dbBw7xkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?**"
      ],
      "metadata": {
        "id": "1vCH5Hpn8FZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **ANOVA (Analysis of Variance)**, the **total variance** of the data is partitioned into two components:\n",
        "\n",
        "### 1. **Between-Group Variance (Variation)**:\n",
        "   - This represents the **variance** due to the differences between the **means of the groups**. It shows how much the group means differ from the overall mean of all observations.\n",
        "   - The formula for **between-group variance** (\\(SS_{\\text{between}}\\)) is calculated as:\n",
        "     \\[\n",
        "     SS_{\\text{between}} = \\sum_{i=1}^{k} n_i (\\bar{X}_i - \\bar{X})^2\n",
        "     \\]\n",
        "     where:\n",
        "     - \\(n_i\\) is the number of observations in group \\(i\\),\n",
        "     - \\(\\bar{X}_i\\) is the mean of group \\(i\\),\n",
        "     - \\(\\bar{X}\\) is the overall mean of all observations,\n",
        "     - \\(k\\) is the number of groups.\n",
        "   \n",
        "### 2. **Within-Group Variance (Variation)**:\n",
        "   - This represents the **variance** due to differences within each group. It accounts for the variation of individual observations around their respective group means.\n",
        "   - The formula for **within-group variance** (\\(SS_{\\text{within}}\\)) is calculated as:\n",
        "     \\[\n",
        "     SS_{\\text{within}} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X}_i)^2\n",
        "     \\]\n",
        "     where:\n",
        "     - \\(X_{ij}\\) is the individual observation in group \\(i\\),\n",
        "     - \\(\\bar{X}_i\\) is the mean of group \\(i\\),\n",
        "     - \\(n_i\\) is the number of observations in group \\(i\\).\n",
        "\n",
        "### **Total Variance**:\n",
        "The total variance (\\(SS_{\\text{total}}\\)) is the variance of all data points around the overall mean:\n",
        "\\[\n",
        "SS_{\\text{total}} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X})^2\n",
        "\\]\n",
        "The total variance is the sum of the between-group and within-group variances:\n",
        "\\[\n",
        "SS_{\\text{total}} = SS_{\\text{between}} + SS_{\\text{within}}\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### **How the Partitioning Contributes to the F-Statistic:**\n",
        "\n",
        "The **F-statistic** is a ratio of the between-group variance to the within-group variance, and it helps determine whether the means of the groups are significantly different.\n",
        "\n",
        "The formula for the **F-statistic** is:\n",
        "\\[\n",
        "F = \\frac{\\text{Mean Square Between}}{\\text{Mean Square Within}}\n",
        "\\]\n",
        "Where:\n",
        "- **Mean Square Between (MSB)** is the average between-group variance, calculated by dividing the sum of squares between groups by its degrees of freedom:\n",
        "  \\[\n",
        "  MSB = \\frac{SS_{\\text{between}}}{df_{\\text{between}}} = \\frac{SS_{\\text{between}}}{k - 1}\n",
        "  \\]\n",
        "  (where \\(k\\) is the number of groups).\n",
        "  \n",
        "- **Mean Square Within (MSW)** is the average within-group variance, calculated by dividing the sum of squares within groups by its degrees of freedom:\n",
        "  \\[\n",
        "  MSW = \\frac{SS_{\\text{within}}}{df_{\\text{within}}} = \\frac{SS_{\\text{within}}}{N - k}\n",
        "  \\]\n",
        "  (where \\(N\\) is the total number of observations).\n",
        "\n",
        "### **Interpretation of the F-Statistic:**\n",
        "- If the **F-statistic** is **large**, it indicates that the between-group variance is much larger than the within-group variance, suggesting that the group means are significantly different.\n",
        "- If the **F-statistic** is **close to 1**, it suggests that the between-group variance is similar to the within-group variance, implying that there is no significant difference between the group means.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary:**\n",
        "The partitioning of variance in ANOVA into between-group and within-group variance allows us to understand how much of the total variation is due to differences between groups (which is of primary interest) and how much is due to random variation within groups. The **F-statistic** then compares these two variances to assess whether the group means are significantly different."
      ],
      "metadata": {
        "id": "7Je3lABu8NfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?**"
      ],
      "metadata": {
        "id": "Q3aFYdN59Cxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **classical (frequentist)** and **Bayesian** approaches to ANOVA differ fundamentally in how they handle **uncertainty**, **parameter estimation**, and **hypothesis testing**. Here's a comparison of the two approaches:\n",
        "\n",
        "### 1. **Handling Uncertainty:**\n",
        "\n",
        "- **Frequentist Approach (Classical ANOVA):**\n",
        "  - The **frequentist** approach treats **parameters** (e.g., group means, variances) as **fixed but unknown values**. Uncertainty about these parameters is captured through sampling distributions and statistical inference.\n",
        "  - **P-values** are used to quantify the **probability of observing the data (or something more extreme) given the null hypothesis**.\n",
        "  - The focus is on **long-run frequencies**—how often results like the current sample would occur if the experiment were repeated many times.\n",
        "\n",
        "- **Bayesian Approach:**\n",
        "  - The **Bayesian** approach treats **parameters** as **random variables** that have a **probability distribution**, representing our **beliefs** about the parameters based on prior knowledge and the observed data.\n",
        "  - **Posterior distributions** are used to represent the uncertainty about the parameters, incorporating both the **prior** (existing knowledge) and the **likelihood** (data observed).\n",
        "  - The focus is on updating beliefs in light of new data and making predictions about future data or parameters.\n",
        "\n",
        "### 2. **Parameter Estimation:**\n",
        "\n",
        "- **Frequentist Approach:**\n",
        "  - Parameters are estimated through **point estimates**, typically using methods like **maximum likelihood estimation** (MLE). In ANOVA, we estimate the group means and variances.\n",
        "  - The **confidence intervals** provide a range of plausible values for the parameters based on the data, but these intervals reflect uncertainty only from the sampling process, not prior beliefs.\n",
        "\n",
        "- **Bayesian Approach:**\n",
        "  - Parameters are estimated using a **posterior distribution**. Instead of a single point estimate, the Bayesian approach provides a **distribution of possible values** for each parameter, reflecting all the uncertainty about that parameter.\n",
        "  - Common summary statistics of the posterior distribution (such as the **mean**, **median**, or **credible intervals**) are used to estimate the parameters.\n",
        "\n",
        "### 3. **Hypothesis Testing:**\n",
        "\n",
        "- **Frequentist Approach:**\n",
        "  - Hypothesis testing in frequentist ANOVA involves testing the **null hypothesis** (that all group means are equal) against the **alternative hypothesis** (that at least one group mean differs).\n",
        "  - The **F-statistic** is computed, and the decision to reject or fail to reject the null hypothesis is based on a **p-value**. A low p-value (typically < 0.05) leads to rejection of the null hypothesis.\n",
        "  - **Type I and Type II errors** are considered in frequentist hypothesis testing, and the approach is focused on the frequency of these errors in repeated sampling.\n",
        "\n",
        "- **Bayesian Approach:**\n",
        "  - Bayesian hypothesis testing doesn't rely on a **p-value**. Instead, it evaluates hypotheses by computing the **posterior probabilities** of different hypotheses given the data.\n",
        "  - The Bayesian approach typically uses **Bayes factors** to compare competing hypotheses. A **Bayes factor** provides evidence in favor of one hypothesis over another.\n",
        "  - **Credible intervals** are used to make inferences about parameters, and a hypothesis can be accepted or rejected based on the degree of belief in the hypothesis given the data.\n",
        "\n",
        "### 4. **Interpretation of Results:**\n",
        "\n",
        "- **Frequentist Approach:**\n",
        "  - Results are typically interpreted in terms of **long-run frequencies**. For instance, a p-value of 0.05 means that, under the null hypothesis, 5% of the time, you would observe data as extreme as the current one.\n",
        "  - Confidence intervals represent the range of values that would contain the true parameter in repeated sampling, but the parameter itself is treated as a fixed value.\n",
        "\n",
        "- **Bayesian Approach:**\n",
        "  - Results are interpreted probabilistically, based on the **posterior distribution**. For instance, a 95% credible interval means there is a 95% probability that the true parameter lies within the interval, given the observed data and prior beliefs.\n",
        "  - The Bayesian approach allows for direct probability statements about the parameters (e.g., \"There is an 80% probability that the mean of Group A is greater than the mean of Group B\").\n",
        "\n",
        "### **Summary of Key Differences:**\n",
        "\n",
        "| Aspect                    | **Frequentist (Classical) Approach**            | **Bayesian Approach**                         |\n",
        "|---------------------------|-----------------------------------------------|----------------------------------------------|\n",
        "| **Handling Uncertainty**   | Parameters are fixed but unknown, uncertainty captured by sampling distributions. | Parameters are random variables, uncertainty captured by the posterior distribution. |\n",
        "| **Parameter Estimation**   | Point estimates (e.g., sample mean, variance) and confidence intervals. | Posterior distributions, providing a range of plausible values for parameters. |\n",
        "| **Hypothesis Testing**     | P-values and F-statistic to test hypotheses, focus on long-run error rates (Type I and II). | Posterior probabilities and Bayes factors, focus on updating beliefs and hypothesis comparison. |\n",
        "| **Interpretation**         | Focus on repeated sampling and long-run frequency properties. | Direct probability statements about parameters, based on prior knowledge and data. |\n",
        "\n",
        "### **When to Use Each Approach:**\n",
        "- **Frequentist ANOVA** is widely used and often preferred in practice, especially when you have large data sets, and computational resources or prior information are limited.\n",
        "- **Bayesian ANOVA** is useful when you have **prior knowledge** or beliefs that you want to incorporate into the analysis, or when you want to make **probabilistic inferences** about parameters. It is also beneficial when working with small sample sizes or when you want to perform **model comparison**.\n",
        "\n",
        "Each approach has its strengths, and the choice between the two depends on the specific context of the analysis and the researcher's preferences."
      ],
      "metadata": {
        "id": "eLI8IrH49AjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Question: You have two sets of data representing the incomes of two different professions\n",
        "\n",
        "  Profession A: [48, 52, 55, 60, 62]\n",
        "  Profession B: [45, 50, 55, 52, 47]\n",
        "\n",
        "  Perform an F-test to determine if the variances of the two professions' incomes are equal. What are your conclusions based on the F-test? Task: Use Python to calculate the F-statistic and p-value for the given data. Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
      ],
      "metadata": {
        "id": "o5OD2l2c9L7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Given data for incomes\n",
        "profession_a = np.array([48, 52, 55, 60, 62])\n",
        "profession_b = np.array([45, 50, 55, 52, 47])\n",
        "\n",
        "# Calculate the variances of the two professions\n",
        "variance_a = np.var(profession_a, ddof=1)\n",
        "variance_b = np.var(profession_b, ddof=1)\n",
        "\n",
        "# Perform F-test for equality of variances\n",
        "f_statistic, p_value = stats.f_oneway(profession_a, profession_b)\n",
        "\n",
        "# Return the results\n",
        "variance_a, variance_b, f_statistic, p_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_l0-Afy9fUK",
        "outputId": "2288d6a7-dc81-4d1d-8e36-a8422020fd24"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32.8, 15.7, 3.232989690721649, 0.10987970118946545)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data Region A: [160, 162, 165, 158, 164 Region B: [172, 175, 170, 168, 174 Region C: [180, 182, 179, 185, 183 Task: Write Python code to perform the one-way ANOVA and interpret the results Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.**"
      ],
      "metadata": {
        "id": "ExpoGVkB96Rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given data for heights in three regions\n",
        "region_a = np.array([160, 162, 165, 158, 164])\n",
        "region_b = np.array([172, 175, 170, 168, 174])\n",
        "region_c = np.array([180, 182, 179, 185, 183])\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "# Return the results\n",
        "f_statistic, p_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d4UVKAO99Zt",
        "outputId": "af9dba42-6d07-4686-cdc9-77ad5aeebcf3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67.87330316742101, 2.870664187937026e-07)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}