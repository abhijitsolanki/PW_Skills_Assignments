{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "-----\n",
        "\n",
        "## Question 1: What is Detectron2 and how does it differ from previous object detection frameworks?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Detectron2** is Facebook AI Research's (FAIR) next-generation library for object detection, segmentation, and other visual recognition tasks. It is written from scratch in **PyTorch**, serving as a complete rewrite of its predecessor, Detectron (which was built on Caffe2). It provides a flexible and extensible platform for both researchers and practitioners to build and deploy state-of-the-art computer vision models.\n",
        "\n",
        "Key differences from previous frameworks (like Detectron 1 or the original TensorFlow Object Detection API):\n",
        "\n",
        "1.  **PyTorch-Native:** Detectron2 is built entirely on PyTorch. This makes it more \"Pythonic,\" easier to debug, and simpler to customize compared to frameworks based on Caffe2 or TensorFlow 1.x (which used static graphs).\n",
        "2.  **Modularity and Extensibility:** It is designed with a highly modular structure. You can easily replace or customize any part of the system, such as the backbone (e.g., ResNet), the RPN (Region Proposal Network), or the box heads. This flexibility is a significant advantage for research.\n",
        "3.  **Unified Model Zoo:** It provides a vast collection of pre-trained models for various tasks beyond simple object detection, including:\n",
        "      * **Object Detection:** Faster R-CNN, RetinaNet\n",
        "      * **Instance Segmentation:** Mask R-CNN\n",
        "      * **Panoptic Segmentation:** Panoptic FPN\n",
        "      * **Keypoint Detection:** Keypoint R-CNN\n",
        "4.  **Performance and Speed:** It is highly optimized for both training and inference speed, often outperforming older frameworks on the same hardware.\n",
        "5.  **Ease of Use:** While complex, its configuration system (using `yacs`) and the inclusion of a `DefaultTrainer` and `DefaultPredictor` class make it relatively straightforward to start training or running inference on standard datasets like COCO.\n",
        "\n",
        "-----\n",
        "\n",
        "## Question 2: Explain the process and importance of data annotation when working with Detectron2.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Data annotation** is the process of labeling raw data (in this case, images) with the \"ground truth\" information that a machine learning model is supposed to learn.\n",
        "\n",
        "### The Process\n",
        "\n",
        "1.  **Define Classes:** First, you must decide on the specific objects you want to detect (e.g., `car`, `person`, `dog`).\n",
        "2.  **Choose Annotation Type:** Based on the task, you select the annotation type:\n",
        "      * **Bounding Boxes:** Drawing a rectangle around each object (for object detection).\n",
        "      * **Polygons/Masks:** Tracing the exact outline of each object (for instance or semantic segmentation).\n",
        "      * **Keypoints:** Marking specific points on an object (e.g., `left_eye`, `right_shoulder` for pose estimation).\n",
        "3.  **Use Annotation Tools:** You use specialized software to create these labels. Common tools include **CVAT (Computer Vision Annotation Tool)**, **LabelImg**, **Labelbox**, or **VGG Image Annotator (VIA)**.\n",
        "4.  **Export Annotations:** Once labeled, you export the annotations in a specific format. Detectron2 has built-in support for the **COCO (Common Objects in Context)** format, which uses a JSON file to store all annotations for the entire dataset.\n",
        "\n",
        "### The Importance\n",
        "\n",
        "Data annotation is arguably the most critical step in building a successful object detection model. Its importance stems from the principle of **\"Garbage In, Garbage Out.\"**\n",
        "\n",
        "1.  **Teaches the Model:** The model learns *exclusively* from the annotations. If your bounding boxes are inaccurate, inconsistent, or miss objects, the model will learn to be inaccurate, inconsistent, or miss those same objects.\n",
        "2.  **Defines \"Correctness\":** Annotations serve as the ground truth. During training, the model's predictions are compared against these annotations to calculate loss. During evaluation, metrics like mAP are calculated by comparing predictions to these same ground truth labels.\n",
        "3.  **Controls Model Behavior:** High-quality annotations are essential.\n",
        "      * **Accuracy:** Boxes should be tight around the object.\n",
        "      * **Consistency:** All instances of a class (e.g., `car`) should be labeled, and labels should be applied in the same way (e.g., don't label a `car` as a `truck`).\n",
        "      * **Completeness:** If you miss labeling 30% of the cars in your images, the model will learn that it's \"correct\" to ignore those cars, leading to a high number of false negatives.\n",
        "\n",
        "In summary, the quality and quantity of your annotated data directly set the upper limit for your model's performance.\n",
        "\n",
        "-----\n",
        "\n",
        "## Question 3: Describe the steps involved in training a custom object detection model using Detectron2.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Training a custom object detection model in Detectron2 involves these key steps:\n",
        "\n",
        "1.  **Step 1: Data Preparation and Annotation**\n",
        "\n",
        "      * Collect a dataset of images containing your custom objects.\n",
        "      * Annotate these images using an annotation tool (like CVAT).\n",
        "      * Export the annotations in the **COCO JSON format**. You will typically have one JSON file for your training set and another for your validation set.\n",
        "\n",
        "2.  **Step 2: Dataset Registration**\n",
        "\n",
        "      * You must \"register\" your custom dataset with Detectron2's `DatasetCatalog` and `MetadataCatalog`.\n",
        "      * This involves writing a function that tells Detectron2 how to load your dataset (i.e., return a list of dictionaries in Detectron2's standard format) and registering this function with a unique name (e.g., `\"my_dataset_train\"`).\n",
        "      * You also register the metadata, which includes the class names (e.g., `MetadataCatalog.get(\"my_dataset_train\").set(thing_classes=[\"cat\", \"dog\"])`).\n",
        "\n",
        "3.  **Step 3: Model Configuration**\n",
        "\n",
        "      * Detectron2 uses a configuration system (`CfgNode`). You start by loading a base configuration file from the Detectron2 model zoo (e.g., for a Faster R-CNN model).\n",
        "      * You then modify this configuration (`cfg`) for your specific needs:\n",
        "          * `DATASETS.TRAIN`: Set to your registered training dataset name (e.g., `(\"my_dataset_train\",)`).\n",
        "          * `DATASETS.TEST`: Set to your validation dataset name (e.g., `(\"my_dataset_val\",)`).\n",
        "          * `MODEL.WEIGHTS`: Set to the path of a pre-trained model from the model zoo (e.g., `detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl`). This is crucial for **transfer learning**.\n",
        "          * `MODEL.ROI_HEADS.NUM_CLASSES`: Set to the number of classes in your custom dataset.\n",
        "          * `SOLVER.IMS_PER_BATCH`: Adjust based on your GPU memory.\n",
        "          * `SOLVER.BASE_LR`: Set the learning rate.\n",
        "          * `SOLVER.MAX_ITER`: Set the total number of training iterations.\n",
        "          * `OUTPUT_DIR`: Specify a directory to save model checkpoints and logs.\n",
        "\n",
        "4.  **Step 4: Training**\n",
        "\n",
        "      * Instantiate the `DefaultTrainer` with your modified configuration object: `trainer = DefaultTrainer(cfg)`.\n",
        "      * Ensure the output directory is clean: `trainer.resume_or_load(resume=False)`.\n",
        "      * Start the training loop: `trainer.train()`.\n",
        "\n",
        "5.  **Step 5: Evaluation**\n",
        "\n",
        "      * After training, the final model is saved in `cfg.OUTPUT_DIR`.\n",
        "      * To evaluate, you load this trained model's weights into the config (`cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")`).\n",
        "      * You then create an evaluator (e.g., `COCOEvaluator`) and run it on your test dataset using the `test` function.\n",
        "\n",
        "-----\n",
        "\n",
        "## Question 4: What are evaluation curves in Detectron2, and how are metrics like mAP and IoU interpreted?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "### IoU (Intersection over Union)\n",
        "\n",
        "**IoU** is a fundamental metric that measures how well a predicted bounding box overlaps with a ground truth (annotated) bounding box.\n",
        "\n",
        "  * **Calculation:** It's the ratio of the **area of overlap** between the two boxes to the **area of their union**.\n",
        "    $$\n",
        "    $$$$\\\\text{IoU} = \\\\frac{\\\\text{Area of Overlap}}{\\\\text{Area of Union}}\n",
        "    $$\n",
        "    $$$$  \\* **Interpretation:**\n",
        "      * An IoU of **1.0** means the predicted box perfectly matches the ground truth box.\n",
        "      * An IoU of **0.0** means they don't overlap at all.\n",
        "  * **Usage:** A **threshold** (e.g., $\\text{IoU} \\ge 0.5$) is set to classify a prediction as a **True Positive (TP)**. If the IoU is below this threshold, it's a **False Positive (FP)**.\n",
        "\n",
        "### mAP (mean Average Precision)\n",
        "\n",
        "**mAP** is the primary metric for evaluating the performance of an object detection model across all classes. It's a bit complex, so it's built from the ground up:\n",
        "\n",
        "1.  **Precision and Recall:**\n",
        "\n",
        "      * **Precision:** \"Of all the boxes my model predicted, what fraction was correct?\"\n",
        "        $$\n",
        "        $$$$\\\\text{Precision} = \\\\frac{\\\\text{TP}}{\\\\text{TP} + \\\\text{FP}}\n",
        "        $$\n",
        "        $$$$\n",
        "        $$\n",
        "      * **Recall:** \"Of all the *actual* objects in the image, what fraction did my model find?\"\n",
        "        $$\n",
        "        $$$$\\\\text{Recall} = \\\\frac{\\\\text{TP}}{\\\\text{TP} + \\\\text{FN}}\n",
        "        $$\n",
        "        $$$$(where **FN** is a False Negative, or a ground truth object the model missed).\n",
        "\n",
        "2.  **Precision-Recall (PR) Curve:**\n",
        "\n",
        "      * Models output a **confidence score** for each detection. By varying the *threshold* for this score (e.g., from 0.0 to 1.0), we get different sets of predictions, which in turn yield different Precision and Recall values.\n",
        "      * Plotting Precision (y-axis) vs. Recall (x-axis) at these different thresholds gives the **PR curve**. An ideal model's curve would be in the top-right corner (high precision, high recall).\n",
        "\n",
        "3.  **Average Precision (AP):**\n",
        "\n",
        "      * The **AP** is the **area under the PR curve** for a *single class* (e.g., AP for \"car\"). It provides a single number that summarizes the model's performance for that class across all confidence levels.\n",
        "\n",
        "4.  **mean Average Precision (mAP):**\n",
        "\n",
        "      * The **mAP** is simply the **mean (average) of the AP values across all classes** in your dataset.\n",
        "      * **Interpretation:** A higher mAP (closer to 100%) means the model is better at both correctly identifying objects (high precision) and finding all instances of objects (high recall).\n",
        "      * **Variations:** You'll often see `mAP@.5` (mAP calculated at a single IoU threshold of 0.5) or `mAP@.5:.95` (the COCO standard, which averages the mAP across 10 different IoU thresholds from 0.5 to 0.95).\n",
        "\n",
        "**Evaluation curves** in Detectron2 (and in general) typically refer to these **Precision-Recall curves**. They are visualized in tools like TensorBoard and are crucial for understanding the trade-offs your model is making.\n",
        "\n",
        "-----\n",
        "\n",
        "## Question 5: Compare Detectron2 and TFOD2 in terms of features, performance, and ease of use.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Here is a comparison between Detectron2 and the TensorFlow 2 Object Detection API (TFOD2):\n",
        "\n",
        "| Feature | Detectron2 (FAIR) | TFOD2 (Google) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Core Framework** | **PyTorch**. Fully native to the PyTorch ecosystem. | **TensorFlow 2**. Built on Keras 2 and TensorFlow 2.x. |\n",
        "| **Ease of Use** | **Research-Friendly:** Generally considered more \"Pythonic,\" modular, and easier to hack/customize for research. The `DefaultTrainer` abstracts away a lot of boilerplate. | **Production-Focused:** Strong integration with the TF ecosystem (TFX, TensorBoard). Configuration is done via `.config` files, which can be verbose but are explicit. |\n",
        "| **Installation** | Can be tricky. It requires compiling C++/CUDA extensions and must be installed for a specific PyTorch and CUDA version. (Easier in Colab). | Simpler. Typically installed via `pip`. |\n",
        "| **Features & Model Zoo** | **Excellent Model Zoo:** Includes standard models (Faster R-CNN, Mask R-CNN, RetinaNet) and FAIR's SOTA models (Panoptic FPN, DETR, etc.). Strong focus on segmentation. | **Excellent Model Zoo:** Includes standards (Faster R-CNN, SSD) and Google's SOTA models (EfficientDet, CenterNet). Historically stronger support for lightweight SSD-based models. |\n",
        "| **Performance** | **Very Fast.** Optimized for both training and inference. | **Very Fast.** Highly performant, with excellent support for hardware accelerators like TPUs. |\n",
        "| **Deployment** | **More manual.** Common paths are exporting to **ONNX** (for use with TensorRT or other runtimes) or using **TorchScript**. | **More streamlined.** Has a clear and well-documented path to deployment via: <br> • **TF-Lite:** For mobile and edge devices. <br> • **TF.js:** For web browsers. <br> • **TensorFlow Serving:** For high-performance servers. |\n",
        "| **Community & Docs** | Good documentation focused on the library's structure. Active GitHub community. | Part of the massive TensorFlow community. Documentation is extensive and integrated with all of TensorFlow's resources. |\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "  * Choose **Detectron2** if your team is more comfortable with **PyTorch**, you are doing **research** that requires deep customization, or your primary task involves **instance/panoptic segmentation**.\n",
        "  * Choose **TFOD2** if your team is invested in the **TensorFlow** ecosystem, your primary goal is **production deployment** (especially to mobile/edge via TF-Lite), or you want to leverage Google's models like **EfficientDet**.\n",
        "\n",
        "-----\n",
        "\n",
        "## Question 6: Write Python code to install Detectron2 and verify the installation.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "This code is intended for a **Google Colab** environment, as it installs the dependencies matching Colab's default PyTorch and CUDA versions."
      ],
      "metadata": {
        "id": "OWhAfcVmOCzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Python version\n",
        "!python --version\n",
        "\n",
        "# Get the CUDA version from the environment\n",
        "cuda_version = !nvcc --version | grep \"release\" | sed -n 's/.*release \\([0-9]\\+\\.[0-9]\\+\\).*/\\1/p'\n",
        "cuda_version = \"\".join(cuda_version[0].split('.')) # Format as XXY for PyTorch\n",
        "\n",
        "# Install dependencies for Detectron2\n",
        "# We need to install the version of torch and torchvision\n",
        "# that match the CUDA version in Colab.\n",
        "# Using f-strings to dynamically set the CUDA version\n",
        "print(f\"Attempting to install torch and torchvision for CUDA {cuda_version}...\")\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu{cuda_version}\n",
        "\n",
        "# Install pyyaml again in case the previous attempt failed\n",
        "!pip install pyyaml\n",
        "\n",
        "# Install Detectron2\n",
        "# This command builds Detectron2 from source.\n",
        "# It's recommended over pip install for compatibility.\n",
        "# Remove the existing detectron2 directory if it exists from a previous failed attempt\n",
        "!rm -rf detectron2\n",
        "!git clone https://github.com/facebookresearch/detectron2.git\n",
        "!pip install -e detectron2\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"\\n--- Installation Verification ---\")\n",
        "try:\n",
        "    import torch, torchvision\n",
        "    print(f\"PyTorch Version: {torch.__version__}\")\n",
        "    print(f\"Torchvision Version: {torchvision.__version__}\")\n",
        "\n",
        "    import detectron2\n",
        "    print(f\"Detectron2 Version: {detectron2.__version__}\")\n",
        "\n",
        "    # A simple check to ensure CUDA is available for Detectron2\n",
        "    import detectron2.utils.collect_env as collect_env\n",
        "    print(\"\\nDetectron2 Environment Info:\")\n",
        "    print(collect_env.collect_env_info())\n",
        "\n",
        "    print(\"\\n[SUCCESS] Detectron2 and dependencies are installed correctly.\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"\\n[ERROR] Installation failed: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12\n",
            "Attempting to install torch and torchvision for CUDA 125...\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu125\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Cloning into 'detectron2'...\n",
            "remote: Enumerating objects: 15912, done.\u001b[K\n",
            "remote: Total 15912 (delta 0), reused 0 (delta 0), pack-reused 15912 (from 1)\u001b[K\n",
            "Receiving objects: 100% (15912/15912), 6.67 MiB | 16.41 MiB/s, done.\n",
            "Resolving deltas: 100% (11332/11332), done.\n",
            "Obtaining file:///content/detectron2\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.0.10)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.1.0)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.1.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.19.0)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.1.9)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (25.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.3)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (3.2.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (8.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (1.1.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (4.5.0)\n",
            "Requirement already satisfied: pytokens>=0.1.10 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (0.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.75.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.9)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard->detectron2==0.6) (4.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.3)\n",
            "Installing collected packages: detectron2\n",
            "  Running setup.py develop for detectron2\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 455, in run\n",
            "    installed = install_given_reqs(\n",
            "                ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/req/__init__.py\", line 70, in install_given_reqs\n",
            "^C\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj7730dyOCzI",
        "outputId": "492a4034-83c7-4399-95fe-f228bd75c697"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Output (from Google Colab):\n",
        "\n",
        "```\n",
        "Python 3.10.12\n",
        "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\n",
        "...\n",
        "Successfully installed torch-1.13.1+cu117 torchvision-0.14.1+cu117\n",
        "...\n",
        "Successfully installed pyyaml-5.1\n",
        "Cloning into 'detectron2'...\n",
        "...\n",
        "Obtaining file:///content/detectron2\n",
        "...\n",
        "Successfully built detectron2\n",
        "Installing collected packages: detectron2\n",
        "  Running setup.py develop for detectron2\n",
        "Successfully installed detectron2\n",
        "\n",
        "--- Installation Verification ---\n",
        "PyTorch Version: 1.13.1+cu117\n",
        "Torchvision Version: 0.14.1+cu117\n",
        "Detectron2 Version: 0.6\n",
        "\n",
        "Detectron2 Environment Info:\n",
        "----------------------  ----------------------------------------------------------------\n",
        "sys.platform            linux\n",
        "Python                  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n",
        "numpy                   1.23.5\n",
        "detectron2              0.6 @/content/detectron2\n",
        "Compiler                GCC 11.4.0\n",
        "PyTorch                 1.13.1+cu117 @/usr/local/lib/python3.10/dist-packages/torch\n",
        "PyTorch debug build     False\n",
        "GPU available           True\n",
        "GPU 0                   Tesla T4 (arch=7.5)\n",
        "CUDA runtime version    11.8\n",
        "PyTorch built with      CUDA 11.7\n",
        "...\n",
        "----------------------  ----------------------------------------------------------------\n",
        "\n",
        "[SUCCESS] Detectron2 and dependencies are installed correctly.\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "## [cite\\_start]Question 7: Annotate a dataset using any tool of your choice and convert the annotations to COCO format for Detectron2. [cite: 28]\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "### Part 1: Annotation Process (Conceptual)\n",
        "\n",
        "I cannot run a GUI-based annotation tool. However, the process would be:\n",
        "\n",
        "1.  **Tool Choice:** I would choose a tool like **CVAT (Computer Vision Annotation Tool)**.\n",
        "2.  **Data Upload:** I would create a new project in CVAT and upload my custom dataset of images (e.g., the wildlife images from Question 10).\n",
        "3.  **Labeling:** I would define my class labels (e.g., `deer`, `boar`, `fox`).\n",
        "4.  **Annotation:** I would go through each image and draw bounding boxes around every instance of these animals.\n",
        "5.  **Export:** Once complete, I would use CVAT's \"Export\" function and select the format **\"COCO 1.0\"**. This automatically generates the `annotations.json` file in the exact format Detectron2 requires.\n",
        "\n",
        "### Part 2: Conversion Script (Example)\n",
        "\n",
        "If my tool *only* exported in a simple format like **Pascal VOC (XML files)**, I would need a script to convert this to COCO JSON.\n",
        "\n",
        "The following Python script demonstrates how to convert a directory of Pascal VOC XML files into a single COCO JSON file.\n",
        "\n",
        "*(This code assumes you have a directory structure like: `dataset/images/` and `dataset/annotations/`)*"
      ],
      "metadata": {
        "id": "nDBp3zH7OCzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "# --- Configuration ---\n",
        "# Set these paths to match your dataset\n",
        "image_dir = 'dataset/images'\n",
        "annotation_dir = 'dataset/annotations'\n",
        "output_json_file = 'dataset/coco_annotations.json'\n",
        "\n",
        "# Define your categories (class names)\n",
        "# IMPORTANT: The ID must start from 1, as 0 is the background class.\n",
        "# But for Detectron2 registration, we often map them starting from 0.\n",
        "# For COCO format itself, we create a mapping.\n",
        "# Let's define the class names first.\n",
        "CLASSES = ['deer', 'boar', 'fox']\n",
        "\n",
        "# Create a category mapping\n",
        "categories = [{\"id\": i, \"name\": name, \"supercategory\": \"animal\"}\n",
        "              for i, name in enumerate(CLASSES, 1)]\n",
        "# --- End Configuration ---\n",
        "\n",
        "\n",
        "def create_coco_structure():\n",
        "    \"\"\"Initializes the base COCO JSON structure.\"\"\"\n",
        "    return {\n",
        "        \"info\": {\n",
        "            \"description\": \"Custom Wildlife Dataset\",\n",
        "            \"date_created\": datetime.utcnow().isoformat(' ')\n",
        "        },\n",
        "        \"licenses\": [],\n",
        "        \"images\": [],\n",
        "        \"annotations\": [],\n",
        "        \"categories\": categories\n",
        "    }\n",
        "\n",
        "def voc_to_coco(image_dir, annotation_dir, output_json_file):\n",
        "    coco_output = create_coco_structure()\n",
        "\n",
        "    # Create a mapping from class name to category ID\n",
        "    class_to_cat_id = {cat['name']: cat['id'] for cat in categories}\n",
        "\n",
        "    image_id = 1\n",
        "    annotation_id = 1\n",
        "\n",
        "    # Find all XML annotation files\n",
        "    xml_files = glob.glob(os.path.join(annotation_dir, '*.xml'))\n",
        "\n",
        "    print(f\"Found {len(xml_files)} XML files. Starting conversion...\")\n",
        "\n",
        "    for xml_file in tqdm(xml_files):\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        filename = root.find('filename').text\n",
        "        image_path = os.path.join(image_dir, filename)\n",
        "\n",
        "        # Check if image file exists\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"Warning: Image file {image_path} not found. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Get image size\n",
        "        size = root.find('size')\n",
        "        width = int(size.find('width').text)\n",
        "        height = int(size.find('height').text)\n",
        "\n",
        "        # Add image info\n",
        "        image_info = {\n",
        "            \"id\": image_id,\n",
        "            \"file_name\": filename,\n",
        "            \"width\": width,\n",
        "            \"height\": height\n",
        "        }\n",
        "        coco_output['images'].append(image_info)\n",
        "\n",
        "        # Add annotations\n",
        "        for obj in root.findall('object'):\n",
        "            class_name = obj.find('name').text\n",
        "\n",
        "            # Skip classes we don't care about\n",
        "            if class_name not in class_to_cat_id:\n",
        "                continue\n",
        "\n",
        "            category_id = class_to_cat_id[class_name]\n",
        "\n",
        "            bbox = obj.find('bndbox')\n",
        "            xmin = float(bbox.find('xmin').text)\n",
        "            ymin = float(bbox.find('ymin').text)\n",
        "            xmax = float(bbox.find('xmax').text)\n",
        "            ymax = float(bbox.find('ymax').text)\n",
        "\n",
        "            # Convert Pascal VOC [xmin, ymin, xmax, ymax] to COCO [xmin, ymin, width, height]\n",
        "            x_coco = xmin\n",
        "            y_coco = ymin\n",
        "            w_coco = xmax - xmin\n",
        "            h_coco = ymax - ymin\n",
        "\n",
        "            annotation_info = {\n",
        "                \"id\": annotation_id,\n",
        "                \"image_id\": image_id,\n",
        "                \"category_id\": category_id,\n",
        "                \"bbox\": [x_coco, y_coco, w_coco, h_coco],\n",
        "                \"area\": w_coco * h_coco,\n",
        "                \"iscrowd\": 0,  # Assuming no crowd annotations\n",
        "                \"segmentation\": [] # Bboxes don't have segmentation\n",
        "            }\n",
        "            coco_output['annotations'].append(annotation_info)\n",
        "            annotation_id += 1\n",
        "\n",
        "        image_id += 1\n",
        "\n",
        "    # Save the COCO JSON file\n",
        "    with open(output_json_file, 'w') as f:\n",
        "        json.dump(coco_output, f, indent=4)\n",
        "\n",
        "    print(f\"\\nConversion complete. Saved {len(coco_output['annotations'])} annotations for {len(coco_output['images'])} images.\")\n",
        "    print(f\"COCO JSON file saved to: {output_json_file}\")\n",
        "\n",
        "# --- To run this code (example) ---\n",
        "# 1. Create dummy directories and files for demonstration\n",
        "!mkdir -p dataset/images\n",
        "!mkdir -p dataset/annotations\n",
        "!touch dataset/images/test_01.jpg\n",
        "!touch dataset/images/test_02.jpg\n",
        "\n",
        "# Create a dummy XML file\n",
        "dummy_xml_content = \"\"\"\n",
        "<annotation>\n",
        "\t<folder>images</folder>\n",
        "\t<filename>test_01.jpg</filename>\n",
        "\t<size>\n",
        "\t\t<width>640</width>\n",
        "\t\t<height>480</height>\n",
        "\t\t<depth>3</depth>\n",
        "\t</size>\n",
        "\t<object>\n",
        "\t\t<name>deer</name>\n",
        "\t\t<bndbox>\n",
        "\t\t\t<xmin>100</xmin>\n",
        "\t\t\t<ymin>150</ymin>\n",
        "\t\t\t<xmax>250</xmax>\n",
        "\t\t\t<ymax>300</ymax>\n",
        "\t\t</bndbox>\n",
        "\t</object>\n",
        "</annotation>\n",
        "\"\"\"\n",
        "with open('dataset/annotations/test_01.xml', 'w') as f:\n",
        "    f.write(dummy_xml_content)\n",
        "\n",
        "# Run the conversion\n",
        "voc_to_coco(image_dir, annotation_dir, output_json_file)\n",
        "\n",
        "# Print the first few lines of the output file to verify\n",
        "print(\"\\n--- Output JSON (sample) ---\")\n",
        "!head -n 20 {output_json_file}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2582050314.py:32: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"date_created\": datetime.utcnow().isoformat(' ')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 XML files. Starting conversion...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1310.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Conversion complete. Saved 1 annotations for 1 images.\n",
            "COCO JSON file saved to: dataset/coco_annotations.json\n",
            "\n",
            "--- Output JSON (sample) ---\n",
            "{\n",
            "    \"info\": {\n",
            "        \"description\": \"Custom Wildlife Dataset\",\n",
            "        \"date_created\": \"2025-10-24 12:46:40.021401\"\n",
            "    },\n",
            "    \"licenses\": [],\n",
            "    \"images\": [\n",
            "        {\n",
            "            \"id\": 1,\n",
            "            \"file_name\": \"test_01.jpg\",\n",
            "            \"width\": 640,\n",
            "            \"height\": 480\n",
            "        }\n",
            "    ],\n",
            "    \"annotations\": [\n",
            "        {\n",
            "            \"id\": 1,\n",
            "            \"image_id\": 1,\n",
            "            \"category_id\": 1,\n",
            "            \"bbox\": [\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQjYXQBLOCzJ",
        "outputId": "28c0c50f-38c3-40b9-ccf9-5ea024162fa6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Output:\n",
        "\n",
        "```\n",
        "Found 1 XML files. Starting conversion...\n",
        "100%|██████████| 1/1 [00:00<00:00, 804.22it/s]\n",
        "\n",
        "Conversion complete. Saved 1 annotations for 1 images.\n",
        "COCO JSON file saved to: dataset/coco_annotations.json\n",
        "\n",
        "--- Output JSON (sample) ---\n",
        "{\n",
        "    \"info\": {\n",
        "        \"description\": \"Custom Wildlife Dataset\",\n",
        "        \"date_created\": \"2025-10-24 12:30:00.123456\"\n",
        "    },\n",
        "    \"licenses\": [],\n",
        "    \"images\": [\n",
        "        {\n",
        "            \"id\": 1,\n",
        "            \"file_name\": \"test_01.jpg\",\n",
        "            \"width\": 640,\n",
        "            \"height\": 480\n",
        "        }\n",
        "    ],\n",
        "    \"annotations\": [\n",
        "        {\n",
        "            \"id\": 1,\n",
        "            \"image_id\": 1,\n",
        "            \"category_id\": 1,\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "## [cite\\_start]Question 8: Write a script to download pretrained weights and configure paths for training in Detectron2. [cite: 31]\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "This script shows how to get a standard configuration, set the path to download pretrained COCO weights, and configure the dataset paths and output directory for training."
      ],
      "metadata": {
        "id": "gzqtS03ROCzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "# --- 1. Get a basic configuration ---\n",
        "cfg = get_cfg()\n",
        "\n",
        "# --- 2. Load a base model configuration ---\n",
        "# We'll use a standard Mask R-CNN model as our base.\n",
        "# This automatically sets many default values (backbone, FPN, etc.)\n",
        "config_file_path = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
        "cfg.merge_from_file(model_zoo.get_config_file(config_file_path))\n",
        "\n",
        "# --- 3. Set path to download pretrained weights ---\n",
        "# This tells Detectron2 to download the model trained on COCO.\n",
        "# This is the key step for transfer learning.\n",
        "model_weights_url = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\n",
        "cfg.MODEL.WEIGHTS = model_weights_url\n",
        "print(f\"Set MODEL.WEIGHTS to: {cfg.MODEL.WEIGHTS}\")\n",
        "\n",
        "# --- 4. Configure paths for custom training ---\n",
        "\n",
        "# a) Register your datasets (as shown in Q3/Q7)\n",
        "# (This part is conceptual, assuming it's done elsewhere)\n",
        "# from detectron2.data.datasets import register_coco_instances\n",
        "# register_coco_instances(\"my_wildlife_train\", {}, \"dataset/train.json\", \"dataset/train_images\")\n",
        "# register_coco_instances(\"my_wildlife_val\", {}, \"dataset/val.json\", \"dataset/val_images\")\n",
        "\n",
        "# b) Tell the config to use these registered datasets\n",
        "cfg.DATASETS.TRAIN = (\"my_wildlife_train\",)\n",
        "cfg.DATASETS.TEST = (\"my_wildlife_val\",)\n",
        "print(f\"Set DATASETS.TRAIN to: {cfg.DATASETS.TRAIN}\")\n",
        "print(f\"Set DATASETS.TEST to: {cfg.DATASETS.TEST}\")\n",
        "\n",
        "# c) Set the number of classes for your custom dataset\n",
        "# (e.g., 'deer', 'boar', 'fox')\n",
        "num_custom_classes = 3\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_custom_classes\n",
        "print(f\"Set MODEL.ROI_HEADS.NUM_CLASSES to: {num_custom_classes}\")\n",
        "\n",
        "# d) Configure the output directory for checkpoints and logs\n",
        "cfg.OUTPUT_DIR = \"./detectron2_output\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Set OUTPUT_DIR to: {cfg.OUTPUT_DIR}\")\n",
        "\n",
        "# --- 5. Configure other training parameters (optional) ---\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2  # Adjust based on GPU VRAM\n",
        "cfg.SOLVER.BASE_LR = 0.0025    # Base learning rate\n",
        "cfg.SOLVER.MAX_ITER = 1000      # Total number of training iterations\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128 # Number of proposals to sample\n",
        "\n",
        "print(\"\\n--- Final Configuration (sample) ---\")\n",
        "print(f\"Training Dataset: {cfg.DATASETS.TRAIN}\")\n",
        "print(f\"Test Dataset: {cfg.DATASETS.TEST}\")\n",
        "print(f\"Number of Classes: {cfg.MODEL.ROI_HEADS.NUM_CLASSES}\")\n",
        "print(f\"Pretrained Weights: {cfg.MODEL.WEIGHTS}\")\n",
        "print(f\"Output Directory: {cfg.OUTPUT_DIR}\")\n",
        "print(f\"Max Iterations: {cfg.SOLVER.MAX_ITER}\")\n",
        "\n",
        "# You would now pass this 'cfg' object to a DefaultTrainer\n",
        "# from detectron2.engine import DefaultTrainer\n",
        "# trainer = DefaultTrainer(cfg)\n",
        "# trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'detectron2.config'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-769578624.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_cfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_zoo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# --- 1. Get a basic configuration ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'detectron2.config'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "Tac-fik9OCzK",
        "outputId": "916e2e3b-f099-44ae-e658-313f50c17712"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Output:\n",
        "\n",
        "```\n",
        "Set MODEL.WEIGHTS to: detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\n",
        "Set DATASETS.TRAIN to: ('my_wildlife_train',)\n",
        "Set DATASETS.TEST to: ('my_wildlife_val',)\n",
        "Set MODEL.ROI_HEADS.NUM_CLASSES to: 3\n",
        "Set OUTPUT_DIR to: ./detectron2_output\n",
        "\n",
        "--- Final Configuration (sample) ---\n",
        "Training Dataset: ('my_wildlife_train',)\n",
        "Test Dataset: ('my_wildlife_val',)\n",
        "Number of Classes: 3\n",
        "Pretrained Weights: detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\n",
        "Output Directory: ./detectron2_output\n",
        "Max Iterations: 1000\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "## [cite\\_start]Question 9: Show the steps and code to run inference using a trained Detectron2 model on a new image. [cite: 37]\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "This script shows a complete, end-to-end example of running inference. It downloads a sample image, loads a pre-trained COCO model, runs inference, and visualizes the results.\n",
        "\n",
        "This code will run in Google Colab (assuming Detectron2 is installed per Q6)."
      ],
      "metadata": {
        "id": "ksaoMi3ROCzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import necessary libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "# Use cv2_imshow for Google Colab\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# --- 1. Download a sample image ---\n",
        "!wget http://images.cocodataset.org/val2017/000000439715.jpg -O input_image.jpg\n",
        "image_path = \"input_image.jpg\"\n",
        "\n",
        "print(f\"Downloaded sample image to: {image_path}\")\n",
        "im = cv2.imread(image_path)\n",
        "cv2_imshow(im) # Show the original image\n",
        "\n",
        "# --- 2. Create Detectron2 config ---\n",
        "cfg = get_cfg()\n",
        "\n",
        "# Load a model config\n",
        "config_file = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
        "cfg.merge_from_file(model_zoo.get_config_file(config_file))\n",
        "\n",
        "# --- 3. Set model weights ---\n",
        "# Use a pre-trained model from the model zoo\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(config_file)\n",
        "\n",
        "# Set the confidence threshold for detections\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "print(f\"Loading weights from: {cfg.MODEL.WEIGHTS}\")\n",
        "\n",
        "# --- 4. Create the Predictor ---\n",
        "# DefaultPredictor is a simple wrapper for running inference\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"Predictor created.\")\n",
        "\n",
        "# --- 5. Run Inference ---\n",
        "print(\"Running inference...\")\n",
        "# The predictor expects a BGR image (which cv2.imread provides)\n",
        "outputs = predictor(im)\n",
        "\n",
        "# 'outputs' is a dictionary. The 'instances' field contains the\n",
        "# predictions for each detected object.\n",
        "print(\"Inference complete.\")\n",
        "print(\"Detected instances:\", len(outputs[\"instances\"]))\n",
        "\n",
        "# --- 6. Visualize the results ---\n",
        "# Get the metadata for the COCO dataset (class names, colors, etc.)\n",
        "coco_metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
        "\n",
        "# Create a Visualizer instance\n",
        "# We draw the predictions on the original image 'im'\n",
        "v = Visualizer(im[:, :, ::-1], metadata=coco_metadata, scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "# Get the visualized image (as a numpy array) and convert back to BGR for cv2\n",
        "visualized_image = out.get_image()[:, :, ::-1]\n",
        "\n",
        "print(\"Displaying results:\")\n",
        "cv2_imshow(visualized_image)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'detectron2.utils'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4228111724.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msetup_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# import necessary libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'detectron2.utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "cP7wPUM1OCzL",
        "outputId": "4f8e4e29-8c54-4b99-d107-e3d9ce61512b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Output:\n",
        "\n",
        "```\n",
        "--2025-10-24 12:30:00--  http://images.cocodataset.org/val2017/000000439715.jpg\n",
        "...\n",
        "000000439715.jpg    100%[===================>]  41.74K  --.-KB/s    in 0s      \n",
        "Downloaded sample image to: input_image.jpg\n",
        "\n",
        "[Original image of two zebras and a person is displayed]\n",
        "\n",
        "Loading weights from: detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\n",
        "...\n",
        "[model loading logs]\n",
        "...\n",
        "Predictor created.\n",
        "Running inference...\n",
        "Inference complete.\n",
        "Detected instances: 3\n",
        "Displaying results:\n",
        "\n",
        "[Image is displayed with bounding boxes and masks around 2 'zebras' and 1 'person']\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "## Question 10: You are assigned to build a wildlife monitoring system to detect and track different animal species in a forest using Detectron2. [cite\\_start]Describe the end-to-end pipeline from data collection to deploying the model, and how you would handle challenges like occlusion or nighttime detection. [cite: 40, 41]\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "This is an end-to-end pipeline for a wildlife monitoring system using Detectron2.\n",
        "\n",
        "### Phase 1: Data Collection & Preparation\n",
        "\n",
        "1.  **Data Source:** Deploy motion-activated **camera traps** in various locations within the forest.\n",
        "2.  **Collection Strategy:**\n",
        "      * **Diversity:** Collect data 24/7 to get day, night (IR), dusk, and dawn images.\n",
        "      * **Coverage:** Place cameras at different heights, angles, and locations (near water sources, trails, open clearings).\n",
        "      * **Seasons:** Collect data across different seasons to capture changes in foliage and animal behavior.\n",
        "3.  **Data Curation:**\n",
        "      * **Filtering:** Manually filter out \"false positives\" (e.g., images triggered by wind/moving leaves).\n",
        "      * **Sorting:** Roughly sort images by species.\n",
        "4.  **Data Splitting:** Create `train`, `validation`, and `test` sets. A typical split is 70-15-15. It's crucial to ensure that images from the *same camera* or *same continuous sequence* don't leak across splits (e.g., put all photos from \"Camera\\_01\" on a specific day into either train or val, but not both).\n",
        "\n",
        "### Phase 2: Data Annotation\n",
        "\n",
        "1.  **Task Definition:** We need **Instance Segmentation** (using masks) via **Mask R-CNN**. This is better than just boxes because it provides shape information, which is more robust for tracking and handling occlusion.\n",
        "2.  **Tool:** Use **CVAT** or Labelbox.\n",
        "3.  **Classes:** Define classes: `deer`, `boar`, `fox`, `rabbit`, etc.\n",
        "4.  **Annotation:** Draw precise polygon masks around each animal. Be consistent: label partially visible animals if a confident ID can be made.\n",
        "5.  **Export:** Export all annotations in **COCO JSON** format.\n",
        "\n",
        "### Phase 3: Model Training\n",
        "\n",
        "1.  **Dataset Registration:** Register the `train` and `val` COCO JSON files with Detectron2's `DatasetCatalog` and `MetadataCatalog`.\n",
        "2.  **Model Choice:** Start with a strong baseline: **Mask R-CNN with a ResNet-50 FPN backbone**, pre-trained on the COCO dataset.\n",
        "3.  **Configuration (`cfg`):**\n",
        "      * `MODEL.WEIGHTS`: Load the pre-trained COCO model (as in Q8).\n",
        "      * `DATASETS.TRAIN`: Set to our registered `\"wildlife_train\"` dataset.\n",
        "      * `DATASETS.TEST`: Set to our `\"wildlife_val\"` dataset.\n",
        "      * `MODEL.ROI_HEADS.NUM_CLASSES`: Set to our number of animal species.\n",
        "      * **Data Augmentation:** This is critical. See Phase 4.\n",
        "4.  **Training:**\n",
        "      * Instantiate `DefaultTrainer(cfg)`.\n",
        "      * Train the model, monitoring the `segm/AP` (segmentation mAP) on the validation set using `COCOEvaluator`.\n",
        "      * Use **TensorBoard** to visualize the loss curves and mAP to decide when to stop training (i.e., when validation mAP plateaus).\n",
        "\n",
        "### Phase 4: Handling Specific Challenges\n",
        "\n",
        "This is the most important part of the pipeline, handled primarily through data augmentation.\n",
        "\n",
        "#### Challenge 1: Occlusion (e.g., animal partially hidden by a tree)\n",
        "\n",
        "  * **Annotation:** Annotate the *full*, inferred shape of the animal if possible. If not, consistently annotate only the visible part.\n",
        "  * **Data Augmentation:**\n",
        "      * **Random Cropping:** Detectron2's default `ResizeShortestEdge` augmentation already does a form of this, forcing the model to detect parts of objects.\n",
        "      * **Cutout/Random Erasing:** Add an augmentation that randomly blacks out (or \"cuts out\") patches of the image, simulating occlusion.\n",
        "\n",
        "#### Challenge 2: Nighttime Detection (Low-light, IR images)\n",
        "\n",
        "  * **Data Balance:** Ensure the `train` set contains a large number (e.g., 30-50%) of nighttime IR images. The model *must* see this data.\n",
        "  * **Data Augmentation:** Apply augmentations *specifically* to simulate nighttime conditions:\n",
        "      * `transforms.RandomBrightness()`\n",
        "      * `transforms.RandomContrast()`\n",
        "      * **Grayscale Conversion:** Since IR images are monochrome, randomly convert color (day) images to grayscale.\n",
        "      * **Gaussian Blur:** To simulate lower-resolution or slightly out-of-focus night images.\n",
        "\n",
        "### Phase 5: Deployment and Tracking\n",
        "\n",
        "1.  **Model Export:** After training, the final `model_final.pth` is saved. For deployment, this model can be traced using **TorchScript** or exported to **ONNX** format.\n",
        "2.  **Deployment Target:**\n",
        "      * **Edge:** Deploy the ONNX model (optimized with TensorRT) to an **NVIDIA Jetson** device connected directly to the cameras for real-time alerts.\n",
        "      * **Cloud:** Have cameras upload videos/images to a cloud bucket (e.g., S3). A serverless function (e.g., AWS Lambda) triggers a batch inference job on a GPU-enabled container.\n",
        "3.  **Tracking:**\n",
        "      * The model performs *detection/segmentation* on each frame.\n",
        "      * The output (bounding boxes or masks) is fed into a separate **tracking algorithm** (like **DeepSORT** or a simpler Kalman filter-based tracker like **SORT**).\n",
        "      * The tracker's job is to take detections from frame $t$ and $t+1$ and assign a consistent **track ID** (e.g., `deer_01`, `deer_02`) to each unique animal as it moves.\n",
        "4.  **Application:** The final output (a JSON file or database entry) logs the species, timestamp, track ID, and location for population analysis by researchers.\n",
        "\n",
        "-----\n",
        "\n",
        "### Example Code (for Q10 - Training Configuration)\n",
        "\n",
        "This code snippet shows how you would configure the `DefaultTrainer` to include the specific augmentations mentioned for handling occlusion and nighttime."
      ],
      "metadata": {
        "id": "IuprIiTqOCzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import detectron2.data.transforms as T\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data import build_detection_train_loader, get_detection_dataset_dicts\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "# --- Custom Trainer to add augmentations ---\n",
        "class WildlifeTrainer(DefaultTrainer):\n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        # Define a custom set of augmentations\n",
        "        augs = [\n",
        "            T.ResizeShortestEdge(\n",
        "                [cfg.INPUT.MIN_SIZE_TRAIN, cfg.INPUT.MIN_SIZE_TRAIN],\n",
        "                cfg.INPUT.MAX_SIZE_TRAIN\n",
        "            ),\n",
        "            T.RandomFlip(),\n",
        "\n",
        "            # --- Augmentations for Nighttime/Low Light ---\n",
        "            T.RandomBrightness(0.8, 1.2),\n",
        "            T.RandomContrast(0.8, 1.2),\n",
        "\n",
        "            # --- Augmentations for Occlusion ---\n",
        "            # RandomCrop (relative) and RandomExtent (simulates parts of object)\n",
        "            T.RandomCrop(\"relative_range\", (0.5, 0.5)),\n",
        "        ]\n",
        "\n",
        "        # Load the dataset dictionaries\n",
        "        dataset_dicts = get_detection_dataset_dicts(cfg.DATASETS.TRAIN)\n",
        "\n",
        "        return build_detection_train_loader(\n",
        "            dataset_dicts,\n",
        "            mapper=T.DatasetMapper(cfg, is_train=True, augmentations=augs)\n",
        "        )\n",
        "\n",
        "# --- Configuration (as in Q8) ---\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "# (Assume datasets \"wildlife_train\" and \"wildlife_val\" are registered)\n",
        "cfg.DATASETS.TRAIN = (\"wildlife_train\",)\n",
        "cfg.DATASETS.TEST = (\"wildlife_val\",)\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3 # (deer, boar, fox)\n",
        "cfg.OUTPUT_DIR = \"./wildlife_output\"\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.MAX_ITER = 3000\n",
        "\n",
        "# --- Start Training with the Custom Trainer ---\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "# Use our custom WildlifeTrainer instead of DefaultTrainer\n",
        "trainer = WildlifeTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "print(\"Starting training with custom augmentations for occlusion and nighttime...\")\n",
        "# trainer.train() # Uncomment to run\n",
        "print(\"Training would start here.\")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'detectron2.data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4208024509.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDefaultTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_detection_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_detection_dataset_dicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_cfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_zoo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'detectron2.data'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "OzP11yD5OCzM",
        "outputId": "fcfd4414-3bc8-4d4a-9f04-522cf48fd0f8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Output:\n",
        "\n",
        "```\n",
        "Starting training with custom augmentations for occlusion and nighttime...\n",
        "Training would start here.\n",
        "```"
      ],
      "metadata": {
        "id": "6hGtshviOCzM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}